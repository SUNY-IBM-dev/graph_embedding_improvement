import os
import sys
import torch
import gc
# import pytorch_lightning as pl
import numpy as np
import re
from collections import Counter

'''
Replaced 'code' to 'source' as there's conflict with vscode debugger.
from code.dataprocessor_graphs import LoadGraphs
from code.model import GIN
from code.trainer import TrainModel
'''

sys.path.append("/home/pwakodi1/tabby/Graph_embedding_aka_signal_amplification_files/source")

from source.dataprocessor_graphs import LoadGraphs
from source.model import GIN
from source.trainer_meng_ver import TrainModel

from itertools import product
import pandas as pd
from datetime import datetime

from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import GroupKFold, StratifiedGroupKFold


''' Applying 'Nested Stratified K-fold Cross-Validation' with GAT, GIN to Project-2-Dataset '''
#SKLEARN: https://scikit-learn.org/stable/modules/classes.html#module-sklearn.model_selectio
#         https://scikit-learn.org/0.21/modules/model_evaluation.html#classification-metrics
# splitter-classes
from pickletools import optimize
# from grpc import stream_unary_rpc_method_handler
from sklearn.model_selection import StratifiedKFold, RepeatedStratifiedKFold
# hyperparameter-optimizers
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
# model-validation
from sklearn.model_selection import cross_val_score # evaluate score by cross-validation.
# performance-metrics
from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score 
from sklearn.metrics import make_scorer
# custom-estimator-class
from sklearn.base import BaseEstimator, ClassifierMixin

#GNN-TRAINING
from source.dataprocessor_graphs import LoadGraphs
#from source.model import GAT, GIN, GIN_no_edgefeat, GIN_no_edgefeat_simplified, GCN, GAT_He, GAT_mlp_fed_1gram 
#from source.model import GNNBasic, LocalMeanPool,  SumPool, GlobalMeanPool

#from source.gnn_dynamicgraph import GNN_DynamicGraph__ImplScheme_1

# from source.trainer import TrainModel, get_dataloader

#ETC
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.loader import DataLoader #PW nood to install torch_geometric
torch.set_printoptions(profile="full")
import numpy as np 
import random
from datetime import datetime
import os
import json
import pandas as pd
pd.set_option('display.max_columns', None)
pd.set_option('display.max_colwidth', None)
import re
import gc
from distutils.util import strtobool
from argparse import ArgumentParser

import pprint
from collections import defaultdict
import time
import matplotlib.pyplot as plt

#**********************************************************************************************************************************************************************

taskname_colnames_old = [
   # Based on "TN2int_Revert()" of "/data/d1/jgwak1/tabby/BASELINE_COMPARISONS/Sequential/RF+Ngrams/RFSVM_ngram_flattened_subgraph_only_psh.py"

   "None", #0

   'CLEANUP', #1
   'CLOSE', #2
   'CREATE', #3
   'CREATENEWFILE', #4
   'DELETEPATH',#5
   'DIRENUM',#6
   'DIRNOTIFY',#7
   'FLUSH',#8
   'FSCTL',#9
   'NAMECREATE',#10
   'NAMEDELETE',#11
   'OPERATIONEND',#12
   'QUERYINFO',#13
   'QUERYINFORMATION',#14
   'QUERYEA',#15
   'QUERYSECURITY',#16
   'READ',#17
   'WRITE',#18
   'SETDELETE',#19
   'SETINFORMATION', #20
   'PAGEPRIORITYCHANGE',#21
   'IOPRIORITYCHANGE',#22
   'CPUBASEPRIORITYCHANGE',#23
   'IMAGEPRIORITYCHANGE',#24
   'CPUPRIORITYCHANGE',#25
   'IMAGELOAD',#26
   'IMAGEUNLOAD',#27
   'PROCESSSTOP',#28
   'PROCESSSTART',#29
   'PROCESSFREEZE',#30
   'PSDISKIOATTRIBUTE',#31
   'PSIORATECONTROL',#32 
   'THREADSTART',#33
   'THREADSTOP',#34
   'THREADWORKONBEHALFUPDATE', #35
   'JOBSTART',#36
   'JOBTERMINATE',#37
   'LOSTEVENT',#38
   'PSDISKIOATTRIBUTION',#39
   'RENAME',#40
   'RENAMEPATH',#41
   'THISGROUPOFEVENTSTRACKSTHEPERFORMANCEOFFLUSHINGHIVES',#42(index in final bit vector)

   "UDPIP42_DatasentoverUDPprotocol",  #43
   "UDPIP43_DatareceivedoverUDPprotocol", #44
   "UDPIP49_UDPconnectionattemptfailed", #45

   "TCPIP10_TCPIPDatasent",   # 46
   "TCPIP11_TCPIPDatareceived", # 47
   "TCPIP12_TCPIPConnectionattempted", # 48
   "TCPIP13_TCPIPDisconnectissued", # 49
   "TCPIP14_TCPIPDataretransmitted", # 50
   "TCPIP15_TCPIPConnectionaccepted", # 51
   "TCPIP16_TCPIPReconnectattempted", # 52
   "TCPIP17_TCPIPTCPconnectionattemptfailed", # 53
   "TCPIP18_TCPIPProtocolcopieddataonbehalfofuser", # 54

   "REGISTRY32_CreateKey", # 55
   "REGISTRY33_OpenKey", # 56
   "REGISTRY34_DeleteKey", # 57
   "REGISTRY35_QueryKey", # 58
   "REGISTRY36_SetValueKey", # 59
   "REGISTRY37_DeleteValueKey", # 60
   "REGISTRY38_QueryValueKey", # 61
   "REGISTRY39_EnumerateKey", # 62
   "REGISTRY40_EnumerateValueKey", # 63
   "REGISTRY41_QueryMultipleValueKey", # 64
   "REGISTRY42_SetInformationKey", # 65
   "REGISTRY43_FlushKey", # 66
   "REGISTRY44_CloseKey", # 67
   "REGISTRY45_QuerySecurityKey", # 68
   "REGISTRY46_SetSecurityKey", # 69

   "Else" # 70

]

taskname_colnames = [
    'None_or_empty', #0 (index 0) 
    'Cleanup', #1
    'Close', #2
    'Create', #3
    'CreateNewFile', #4
    'DeletePath',#5
    'DirEnum',#6
    'DirNotify',#7
    'Flush',#8
    'FSCTL',#9
    'NameCreate',#10
    'NameDelete',#11
    'OperationEnd',#12
    #'QueINFO',#13
    'QueryInformation',#13
    'QueryEA',#14
    'QuerySecurity',#15
    'Read',#16
    'Write',#17
    'SetDelete',#18
    'SetInformation', #19
    'PagePriorityChange',#20
    'IoPriorityChange',#21
    'CpuBasePriorityChange',#22
    #'IMAGEPriorityChange',#24
    'CpuPriorityChange',#23
    'ImageLoad',#24
    'ImageUnload',#25
    'ProcessStop/Stop',#26
    'ProcessStart/Start',#27
    'ProcessFreeze/Start',#28--------
    #'PSDISKIOATTRIBUTE',#31
    #'PSIORATECONTROL',#32 
    'ThreadStart/Start',#29
    'ThreadStop/Stop',#30
    'ThreadWorkOnBehalfUpdate', #31
    'JobStart/Start',#32--------
    'JobTerminate/Stop',#33--------
    #'LOSTEVENT',#38
    #'PSDISKIOATTRIBUTION',#39
    'Rename',#34
    'Renamepath',#35
    'Thisgroupofeventstrackstheperformanceofflushinghives',#36-------
    'EventID(1)',#37
    'EventID(2)',#38
    'EventID(3)',#39
    'EventID(4)',#40
    'EventID(5)',#41
    'EventID(6)',#42
    'EventID(7)',#43
    'EventID(8)',#44
    'EventID(9)',#45
    'EventID(10)',#46
    'EventID(11)',#47
    'EventID(13)',#48
    'EventID(14)',#49
    'EventID(15)',#50
    'KERNEL_NETWORK_TASK_TCPIP/Datasent.', #51
    'KERNEL_NETWORK_TASK_TCPIP/Datareceived.',#52
    'KERNEL_NETWORK_TASK_TCPIP/Connectionattempted.',#53
    'KERNEL_NETWORK_TASK_TCPIP/Disconnectissued.', #54
    'KERNEL_NETWORK_TASK_TCPIP/Dataretransmitted.',#55
    'KERNEL_NETWORK_TASK_TCPIP/connectionaccepted.' , #56----
    'KERNEL_NETWORK_TASK_TCPIP/Protocolcopieddataonbehalfofuser.', #57
    'KERNEL_NETWORK_TASK_UDPIP/DatareceivedoverUDPprotocol.',#58
    'KERNEL_NETWORK_TASK_UDPIP/DatasentoverUDPprotocol.', #59
    # 'NULL', #remove this entry once second step updations used for subgra
    'Unseen', # 60
    'File', #61
    'Reg', #62
    'Net', #63
    'Proc', #64
    'Thread', #65
    #PW: below events are now different in silketw
    # all below 3 task are combined with opcode and having index 43 onwards for all of them in the function TN2int()
    # 'KERNEL_NETWORK_TASK_UDPIP'#index 43 # 42(opcode value) 43,49(https://github.com/repnz/etw-providers-docs/blob/master/Manifests-Win7-7600/Microsoft-Windows-Kernel-Network.xml)
    # 'KERNEL_NETWORK_TASK_TCPIP', # 10-18 (https://github.com/repnz/etw-providers-docs/blob/master/Manifests-Win7-7600/Microsoft-Windows-Kernel-Network.xml)
    # 'MICROSOFT-WINDOWS-KERNEL-REGISTRY', # 32- 46 (https://github.com/repnz/etw-providers-docs/blob/master/Manifests-Win7-7600/Microsoft-Windows-Kernel-Registry.xml)

]


#------------------------------------------------------------------------------------------------------------------------#
#------------------------------------------------------------------------------------------------------------------------#
#------------------------------------------------------------------------------------------------------------------------#

# PW 19th Dec,23 below code using for benign flattened graph vs. malware flattened graph comparison
def get_No_Graph_Structure_eventdist_dict( dataset : list ):

      file_node_tensor = torch.tensor([1, 0, 0, 0, 0])
      reg_node_tensor = torch.tensor([0, 1, 0, 0, 0])
      net_node_tensor = torch.tensor([0, 0, 1, 0, 0])
      proc_node_tensor = torch.tensor([0, 0, 0, 1, 0])
      thread_node_tensor = torch.tensor([0, 0, 0, 0, 1])

      data_dict = dict()

      for data in dataset:

         data.edge_attr = data.edge_attr[:,:-1] # drop time-scalar

         eventdist = torch.sum(data.edge_attr, dim = 0)

         result_list = [taskname_colnames[i] if value != 0.0 else value for i, value in enumerate(eventdist.tolist())]


         data_dict[ re.search(r'Processed_SUBGRAPH_P3_(.*)\.pickle', data.name).group(1) ] = result_list

      return data_dict

def get_No_Graph_Structure_eventdist_nodetype5bit_dist_dict( dataset : list ):
      file_node_tensor = torch.tensor([1, 0, 0, 0, 0])
      reg_node_tensor = torch.tensor([0, 1, 0, 0, 0])
      net_node_tensor = torch.tensor([0, 0, 1, 0, 0])
      proc_node_tensor = torch.tensor([0, 0, 0, 1, 0])
      thread_node_tensor = torch.tensor([0, 0, 0, 0, 1])

      data_dict = dict()

      for data in dataset:

         data.x = data.x[:,:5]
         data.edge_attr = data.edge_attr[:,:-1] # drop time-scalar

         eventdist = torch.sum(data.edge_attr, dim = 0)
         nodetype_5bit_dist = torch.sum(data.x, axis = 0)

         event_nodetype5bit_dist = torch.cat(( eventdist, nodetype_5bit_dist ), dim = 0)
         result_list = [taskname_colnames[i] if (value != 0.0 or value != 0) else value for i, value in enumerate(event_nodetype5bit_dist.tolist())]

         data_dict[ re.search(r'Processed_SUBGRAPH_P3_(.*)\.pickle', data.name).group(1) ] = result_list

      return data_dict

def get_signal_amplified_thread_level_eventdist_adjacent_5bit_dist_dict( dataset : list ):
      
      # A little different from "get_thread_level_eventdist_adjacentFRNpatterndist_dict"
      # Differencce is that "get_thread_level_eventdist_adjacentFRNpatterndist_dict" only considers adjacent-nodes Adhoc-pattern-node-attr
      # While this considers the 5Bit+Adhoc-pattern-node-attr

      file_node_tensor = torch.tensor([1, 0, 0, 0, 0])
      reg_node_tensor = torch.tensor([0, 1, 0, 0, 0])
      net_node_tensor = torch.tensor([0, 0, 1, 0, 0])
      proc_node_tensor = torch.tensor([0, 0, 0, 1, 0])
      thread_node_tensor = torch.tensor([0, 0, 0, 0, 1])

      data_dict = dict()

      cnt = 1
      for data in dataset:
            
            print(f"signal-amplifying: {data.name} ||  {cnt}/{len(dataset)}", flush=True)

            # Added by JY @ 2023-07-18 to handle node-attr dim > 5  
            # if data.x.shape[1] != 5:
            data_x_first5 = data.x[:,:5]

            file_node_indices = torch.nonzero(torch.all(torch.eq( data_x_first5, file_node_tensor), dim=1), as_tuple=False).flatten().tolist()
            reg_node_indices = torch.nonzero(torch.all(torch.eq( data_x_first5, reg_node_tensor), dim=1), as_tuple=False).flatten().tolist()
            net_node_indices = torch.nonzero(torch.all(torch.eq( data_x_first5, net_node_tensor), dim=1), as_tuple=False).flatten().tolist()
            proc_node_indices = torch.nonzero(torch.all(torch.eq( data_x_first5, proc_node_tensor), dim=1), as_tuple=False).flatten().tolist()
            thread_node_indices = torch.nonzero(torch.all(torch.eq( data_x_first5, thread_node_tensor), dim=1), as_tuple=False).flatten().tolist()

            # which this node is a source-node (outgoing-edge w.r.t this node)
            
            data_thread_node_incoming_edges_edge_attrs = torch.tensor([])
            data_thread_node_outgoing_edges_edge_attrs = torch.tensor([])
            data_thread_node_both_direction_edges_edge_attrs = torch.tensor([])
            data_thread_node_all_unique_adjacent_nodes_5bit_dists = torch.tensor([]) # Added by JY @ 2023-07-19

            for thread_node_idx in thread_node_indices:

               edge_src_indices = data.edge_index[0]
               edge_tar_indices = data.edge_index[1]

               # which this node is a target-node (outgoing-edge w.r.t this node)
               outgoing_edges_from_thread_node_idx = torch.nonzero( edge_src_indices == thread_node_idx ).flatten()
               # which this node is a target-node (incoming-edge w.r.t this node)
               incoming_edges_to_thread_node_idx = torch.nonzero( edge_tar_indices == thread_node_idx ).flatten()

               # Following is to deal with edge-attr (event-dist & time-scalar) -------------------------------------------------------------------------------------
               edge_attr_of_incoming_edges_from_thread_node_idx = data.edge_attr[incoming_edges_to_thread_node_idx]
               edge_attr_of_outgoing_edges_from_thread_node_idx = data.edge_attr[outgoing_edges_from_thread_node_idx]

               edge_attr_of_incoming_edges_from_thread_node_idx_sum = torch.sum(edge_attr_of_incoming_edges_from_thread_node_idx[:,:-1], dim = 0)
               edge_attr_of_outgoing_edges_from_thread_node_idx_sum = torch.sum(edge_attr_of_outgoing_edges_from_thread_node_idx[:,:-1], dim = 0)
               edge_attr_of_both_direction_edges_from_thread_node_idx_sum = torch.add(edge_attr_of_incoming_edges_from_thread_node_idx_sum, 
                                                                                      edge_attr_of_outgoing_edges_from_thread_node_idx_sum)
               
               data_thread_node_both_direction_edges_edge_attrs = torch.cat(( data_thread_node_both_direction_edges_edge_attrs,
                                                                              edge_attr_of_both_direction_edges_from_thread_node_idx_sum.unsqueeze(0) ), dim = 0)
               # -------------------------------------------------------------------------------------------------------------------------------------------------------

               # JY @ 2023-07-18: Now get all Adhoc identifier-pattern distributions of adjacent F/R/N t

               # But also need to consider the multi-graph aspect here. 
               # So here, do not count twice for duplicate adjacent nodes due to multigraph.
               # Just need to get the distribution.
               # Find unique column-wise pairs (dropping duplicates - src/dst pairs that come from multi-graph)
               unique_outgoing_edges_from_thread_node, _ = torch.unique( data.edge_index[:, outgoing_edges_from_thread_node_idx ], dim=1, return_inverse=True)

               # Find unique column-wise pairs (dropping duplicates - src/dst pairs that come from multi-graph)
               unique_incoming_edges_to_thread_node, _ = torch.unique( data.edge_index[:, incoming_edges_to_thread_node_idx ], dim=1, return_inverse=True)

               target_nodes_of_outgoing_edges_from_thread_node = unique_outgoing_edges_from_thread_node[1] # edge-target is index 1
               source_nodes_of_incoming_edges_to_thread_node = unique_incoming_edges_to_thread_node[0] # edge-src is index 0

               # "5bit 
               data__5bit = data.x[:,:5]

               #-- Option-1 --------------------------------------------------------------------------------------------------------------------------------------------------------------
               # # Already handled multi-graph case, but how about the bi-directional edge case?
               # # For, T-->F and T<--F, information of F will be recorded twice."Dont Let it happen"
               unique_adjacent_nodes_of_both_direction_edges_of_thread_node = torch.unique( torch.cat( [ target_nodes_of_outgoing_edges_from_thread_node, 
                                                                                                         source_nodes_of_incoming_edges_to_thread_node ] ) )
               integrated_5bit_of_all_unique_adjacent_nodes_to_thread = torch.sum( data__5bit[unique_adjacent_nodes_of_both_direction_edges_of_thread_node], dim = 0 )
               
               data_thread_node_all_unique_adjacent_nodes_5bit_dists = torch.cat(( data_thread_node_all_unique_adjacent_nodes_5bit_dists,
                                                                                          integrated_5bit_of_all_unique_adjacent_nodes_to_thread.unsqueeze(0) ), dim = 0)
               # --------------------------------------------------------------------------------------------------------------------------------------------------------------------------

               #-- Option-2 --------------------------------------------------------------------------------------------------------------------------------------------------------------
               # # # Already handled multi-graph case, but how about the bi-directional edge case?
               # # # For, T-->F and T<--F, information of F will be recorded twice. "Let it happen"
               # non_unique_adjacent_nodes_of_both_direction_edges_of_thread_node = torch.cat( [ target_nodes_of_outgoing_edges_from_thread_node, 
               #                                                                                 source_nodes_of_incoming_edges_to_thread_node ] )
               # integrated_5bit_of_all_non_unique_adjacent_nodes_to_thread = torch.sum( data__5bit[non_unique_adjacent_nodes_of_both_direction_edges_of_thread_node], dim = 0 )
               
               # data_thread_node_all_unique_adjacent_nodes_5bit_dists = torch.cat(( data_thread_node_all_unique_adjacent_nodes_5bit_dists,
               #                                                                      integrated_5bit_of_all_non_unique_adjacent_nodes_to_thread.unsqueeze(0) ), dim = 0)



               # --------------------------------------------------------------------------------------------------------------------------------------------------------------------------

            thread_eventdist_adjacent_5bit_dist = torch.cat( [data_thread_node_both_direction_edges_edge_attrs, 
                                                                  data_thread_node_all_unique_adjacent_nodes_5bit_dists], dim = 1)

            result_list = [taskname_colnames[i] if (value != 0.0 or value != 0) else value for i, value in enumerate(thread_eventdist_adjacent_5bit_dist.tolist())]

            data_dict[ re.search(r'Processed_SUBGRAPH_P3_(.*?)\.pickle', data.name).group(1) ] = result_list

            # data_dict[ data.name.lstrip("Processed_SUBGRAPH_P3_").rstrip(".pickle") ] = data_thread_node_both_direction_edges_edge_attrs.tolist()
            cnt+=1
      return data_dict

def get_No_Graph_Structure_eventdist_nodetype5bit_dist_dict_new( dataset : list ):
      file_node_tensor = torch.tensor([1, 0, 0, 0, 0])
      reg_node_tensor = torch.tensor([0, 1, 0, 0, 0])
      net_node_tensor = torch.tensor([0, 0, 1, 0, 0])
      proc_node_tensor = torch.tensor([0, 0, 0, 1, 0])
      thread_node_tensor = torch.tensor([0, 0, 0, 0, 1])

      data_dict = dict()
      total_events_per_sample = []
      for data in dataset:

         data.x = data.x[:,:5]
         data.edge_attr = data.edge_attr[:,:-1] # drop time-scalar
         name = data.name
         edge_attr = data.edge_attr
         total_events = torch.sum(edge_attr, dim=0).int()
         total_events_per_sample.append({'name': name, 'total_events': total_events})      # Save results to a text file
      
      output_file_path = "/home/pwakodi1/tabby/Graph_embedding_aka_signal_amplification_files/Analysis_results/Case1_Malware_testdata_total_events_per_sample.txt"
      with open(output_file_path, 'w') as file:
          for result in total_events_per_sample:
              file.write(f"Sample {result['name']}:\n")
              for task_name, count in zip(taskname_colnames, result['total_events']):
                  file.write(f"{task_name}: {int(count)}\n")
              file.write("\n")
      print(f"Results saved to {output_file_path}")

def summary_malware_benign_events_count():
    malware_event_counts = {}
    benign_event_counts = {}
    
    with open("/home/pwakodi1/tabby/Graph_embedding_aka_signal_amplification_files/Analysis_results/Case1_Malware_testdata_total_events_per_sample.txt", "r") as file:
        current_sample = None
        for line in file:
            line = line.strip()
            if line.startswith("Sample Processed_"):
                current_sample = line
            elif current_sample is not None and ":" in line:
                event, count = line.split(":")
                event = event.strip()
                count = int(count.strip())
                malware_event_counts.setdefault(event, 0)
                malware_event_counts[event] += count
    
    with open("/home/pwakodi1/tabby/Graph_embedding_aka_signal_amplification_files/Analysis_results/Case1_Benign_testdata_total_events_per_sample.txt", "r") as file:
        current_sample = None
        for line in file:
            line = line.strip()
            if line.startswith("Sample Processed_"):
                current_sample = line
            elif current_sample is not None and ":" in line:
                event, count = line.split(":")
                event = event.strip()
                count = int(count.strip())
                benign_event_counts.setdefault(event, 0)
                benign_event_counts[event] += count

    # Print and save the summary
    summary_output_file_path = "/home/pwakodi1/tabby/Graph_embedding_aka_signal_amplification_files/Analysis_results/Case1_Test_malware_benign_event_summary.txt"
    with open(summary_output_file_path, "w") as summary_file:
        summary_file.write("\n **********************MALWARE_EVENT_Count********************** \n")
        for event, total_count in malware_event_counts.items():
            print(f"{event}: {total_count}")
            summary_file.write(f" \" {event} \": {total_count},\n")
        summary_file.write("\n **********************BENIGN_EVENT_Count********************** \n")
        for event, total_count in benign_event_counts.items():
            print(f"{event}: {total_count}")
            summary_file.write(f"\" {event} \": {total_count},\n")

    print(f"Summary saved to {summary_output_file_path}")


def eventdist(benign_data_dict, malware_data_dict):
    from collections import Counter

    # Collect all unique events from both dictionaries
    all_events = set()
    for events_list in benign_data_dict.values():
        all_events.update(events_list)
    for events_list in malware_data_dict.values():
        all_events.update(events_list)

    # Initialize counters for both dictionaries
    counter_dict1 = Counter()
    counter_dict2 = Counter()

    # Count occurrences of each event in each dictionary
    for events_list in benign_data_dict.values():
        counter_dict1.update(events_list)
    for events_list in malware_data_dict.values():
        counter_dict2.update(events_list)

    # Calculate the difference in event counts
    difference = {event: counter_dict1[event] - counter_dict2[event] for event in all_events}

    # Total count of each event in benign samples
    total_count_benign = dict(counter_dict1)

    # Total count of each event in malware samples
    total_count_malware = dict(counter_dict2)

    # Common events with their total counts in both benign and malware samples
    common_events = {event: counter_dict1[event] + counter_dict2[event] for event in all_events if event in counter_dict1 and event in counter_dict2}

    uncommon_events_benign = {event: counter_dict1[event] for event in all_events if event not in counter_dict2}

    # Uncommon events in malware samples
    uncommon_events_malware = {event: counter_dict2[event] for event in all_events if event not in counter_dict1}
    
    return total_count_benign, total_count_malware, common_events, uncommon_events_benign, uncommon_events_malware


def save_and_show_plot(figure, filename):
    plt.savefig(filename, bbox_inches='tight')
    plt.show()

def plot_event_counts(total_count_benign, total_count_malware):
    labels = list(total_count_benign.keys())
    
    # Check if lengths match
    if len(total_count_benign) != len(total_count_malware):
        raise ValueError("Lengths of benign and malware data must be the same.")
    
    positions = np.arange(len(labels))
    width = 0.35

    fig, ax = plt.subplots(figsize=(12, 6))
    ax.bar(positions - width/2, total_count_benign.values(), width, label='Benign', alpha=0.7)
    ax.bar(positions + width/2, total_count_malware.values(), width, label='Malware', alpha=0.7)

    ax.set_xlabel('Event')
    ax.set_ylabel('Count')
    ax.set_title('Comparison of Benign and Malware Events')
    ax.legend()
    ax.set_xticks(positions)
    ax.set_xticklabels(labels, rotation=45)      
    save_and_show_plot(plt, '/home/pwakodi1/tabby/Graph_embedding_aka_signal_amplification_files/Analysis_results/malware_vs_benign_comparison_plot.png')
# Example usage



def get_tasknames_unique_to_class( benign_data_dict, malware_data_dict ):
    
    # Goal : Get the tasknames that uniquely appear in benign or malware (on the dataset level ; all benign samples vs. all malware samples)
    benign_sample_global_set = set()

    for benign_dataname, taskname_frequency_dict in benign_data_dict.items():
       benign_sample_global_set += set(taskname_frequency_dict.keys())

    malware_sample_global_set = set()

    for malware_dataname, taskname_frequency_dict in malware_data_dict.items():
       malware_sample_global_set += set(taskname_frequency_dict.keys())

    tasknames_unique_to_malware = malware_sample_global_set - benign_sample_global_set
    tasknames_unique_to_benign = benign_sample_global_set - malware_sample_global_set 
    tasknames_appear_in_both = benign_sample_global_set.intersection(malware_sample_global_set)

    return tasknames_unique_to_benign, tasknames_unique_to_malware , tasknames_appear_in_both
#**********************************************************************************************************************************************************************

#**********************************************************************************************************************************************************************
#**********************************************************************************************************************************************************************
#**********************************************************************************************************************************************************************

#############################################################################################################################
# libraries for tradition-ML (2023-07-25)

import sklearn
from sklearn import metrics
from sklearn import svm
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import GradientBoostingClassifier # XGB

# https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMClassifier.html#lightgbm.LGBMClassifier

# for baseline
from sklearn.feature_extraction.text import CountVectorizer


##############################################################################################################################



if __name__ == '__main__':
    ###############################################################################################################################################
    # Set data paths
    projection_datapath_Benign_Train_dict = {
      # Dataset-1 (B#288, M#248) ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
      #PW: Dataset-Case-1 
      "Dataset-Case-1": \
         "/home/pwakodi1/tabby/SILKETW_benign_train_test_data_case1/offline_train/Processed_Benign_ONLY_TaskName_edgeattr",

      # ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
      # Dataset-2 (B#662, M#628)
      "Dataset-Case-2": \
         "/home/pwakodi1/tabby/SILKETW_benign_train_test_data_case1_case2/offline_train/Processed_Benign_ONLY_TaskName_edgeattr"
    
    }
    projection_datapath_Malware_Train_dict = {
      # Dataset-1 (B#288, M#248) ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
      "Dataset-Case-1": \
         "/home/pwakodi1/tabby/SILKETW_malware_train_test_data_case1/offline_train/Processed_Malware_ONLY_TaskName_edgeattr",
      # ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
      # Dataset-2 (B#662, M#628)
      "Dataset-Case-2": \
         "/home/pwakodi1/tabby/SILKETW_malware_train_test_data_case1_case2/offline_train/Processed_Malware_ONLY_TaskName_edgeattr"
    }
    projection_datapath_Benign_Test_dict = {
      # Dataset-1 (B#73, M#62) ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
      "Dataset-Case-1": \
         "/home/pwakodi1/tabby/SILKETW_benign_train_test_data_case1/offline_test/Processed_Benign_ONLY_TaskName_edgeattr",
      # ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
      # Dataset-2 (B#167, M#158)
      "Dataset-Case-2": \
         "/home/pwakodi1/tabby/SILKETW_benign_train_test_data_case1_case2/offline_test/Processed_Benign_ONLY_TaskName_edgeattr"

    }
    projection_datapath_Malware_Test_dict = {
      # Dataset-1 (B#73, M#62) ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
      "Dataset-Case-1": \
         "/home/pwakodi1/tabby/SILKETW_malware_train_test_data_case1/offline_test/Processed_Malware_ONLY_TaskName_edgeattr",
      # ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
      # Dataset-2 (B#167, M#158)
      "Dataset-Case-2": \
         "/home/pwakodi1/tabby/SILKETW_malware_train_test_data_case1_case2/offline_test/Processed_Malware_ONLY_TaskName_edgeattr"


    }

    _dim_node = 5 #46   # num node features ; the #feats
    _dim_edge = 62 #72    # (or edge_dim) ; num edge features

    _benign_train_data_path = projection_datapath_Benign_Train_dict["Dataset-Case-1"]
    _malware_train_data_path = projection_datapath_Malware_Train_dict["Dataset-Case-1"]
    _benign_final_test_data_path = projection_datapath_Benign_Test_dict["Dataset-Case-1"]
    _malware_final_test_data_path = projection_datapath_Malware_Test_dict["Dataset-Case-1"]



    # Load both benign and malware graphs """
    #dataprocessor = LoadGraphs()
   #  benign_train_dataset = dataprocessor.parse_all_data(_benign_train_data_path, _dim_node, _dim_edge, drop_dim_edge_timescalar = False)
   #  malware_train_dataset = dataprocessor.parse_all_data(_malware_train_data_path, _dim_node, _dim_edge, drop_dim_edge_timescalar = False)
    
    #train_dataset = benign_train_dataset + malware_train_dataset
    #print('+ train data loaded #Malware = {} | #Benign = {}'.format(len(malware_train_dataset), len(benign_train_dataset)), flush=True)

    # Load test benign and malware graphs """
   #  benign_test_dataset = dataprocessor.parse_all_data(_benign_final_test_data_path, _dim_node, _dim_edge, drop_dim_edge_timescalar = False)
   #  malware_test_dataset = dataprocessor.parse_all_data(_malware_final_test_data_path, _dim_node, _dim_edge, drop_dim_edge_timescalar = False)
    
    #final_test_dataset = benign_test_dataset + malware_test_dataset
   # print('+ final-test data loaded #Malware = {} | #Benign = {}'.format(len(malware_test_dataset), len(benign_test_dataset)), flush=True)



    #benign_train_dataset__no_graph_structure_dict = get_No_Graph_Structure_eventdist_nodetype5bit_dist_dict_new( dataset= benign_train_dataset )  
    #malware_train_dataset__no_graph_structure_dict = get_No_Graph_Structure_eventdist_nodetype5bit_dist_dict_new( dataset= malware_train_dataset )  
   #  total_eventcount_benign, total_eventcount_malware,common_events, uncommon_events_benign, uncommon_events_malware = eventdist(benign_train_dataset__no_graph_structure_dict,malware_train_dataset__no_graph_structure_dict)
   #  with open('/home/pwakodi1/tabby/Graph_embedding_aka_signal_amplification_files/Analysis_results/benign_vs_malware_train_eventdist_5bit_result_case1.json', 'w') as file:
      # json.dump({'total_eventcount_benign': total_eventcount_benign, 'total_eventcount_malware': total_eventcount_malware, 'common_events':common_events, 'uncommon_events_benign': uncommon_events_benign, 'uncommon_events_malware': uncommon_events_malware}, file)
    
    #benign_test_dataset__no_graph_structure_dict = get_No_Graph_Structure_eventdist_nodetype5bit_dist_dict_new( dataset= benign_test_dataset )  
   #  malware_test_dataset__no_graph_structure_dict = get_No_Graph_Structure_eventdist_nodetype5bit_dist_dict_new( dataset= malware_test_dataset )  
   #  total_eventcount_benign, total_eventcount_malware, common_events, uncommon_events_benign, uncommon_events_malware = eventdist(benign_test_dataset__no_graph_structure_dict,malware_test_dataset__no_graph_structure_dict)
   #  with open('/home/pwakodi1/tabby/Graph_embedding_aka_signal_amplification_files/Analysis_results/benign_vs_malware_test_eventdist_5bit_result_case1.json', 'w') as file:
      # json.dump({'total_eventcount_benign': total_eventcount_benign, 'total_eventcount_malware': total_eventcount_malware, 'common_events':common_events,  'uncommon_events_benign': uncommon_events_benign, 'uncommon_events_malware': uncommon_events_malware}, file)
    
    #plot_event_counts(total_eventcount_benign, total_eventcount_malware)

    #print(difference_dict)
    summary_malware_benign_events_count()