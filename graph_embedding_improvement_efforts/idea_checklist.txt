[ Ideas to try:]

-----------------------------------------------------------------------------------------------------------------------------------
[Priority-1] TRY: Prof Guanhua said to consider “concatenating” the temporal-Ngram with graph-embedding? 
                  Graph-embedding serving as additional info.  
                  Then, in this case, baseline, should be the corresponding temporal-Ngram
   — “could” write a script : /data/d1/jgwak1/tabby/graph_embedding_improvement_JY_git/graph_embedding_improvement/concat_ngram_and_graph_embedding.py
   
         which integrates the following the “n-gram” part with “graph-embedding” part, referring to:
         -- /data/d1/jgwak1/tabby/graph_embedding_improvement_JY_git/analyze_at_model_explainer/analyze_RF_with_explainer.py
         -- /data/d1/jgwak1/tabby/graph_embedding_improvement_JY_git/N_gram_hyperparameter_tuning/n_gram_hypeparam_tuning.py

-----------------------------------------------------------------------------------------------------------------------------------
[Priority-2] Weighted-sum
   - Give lower weights on artifactual threads (~ give higher weights to more important threads )
   
   --> So what is the logic behind process starting threads?
   
   --> If a process runs a particular application, is there a straightforward way to distinguish threads that correspond to the actual activity of that application and threads that are more for background activities ?
      ^-- When looked directly into the Threadstart event ETW log-entries, and didn't see any interesting pattern.

   --> Analyze : /data/d1/jgwak1/tabby/graph_embedding_improvement_JY_git/graph_embedding_improvement_efforts/Trial_2__Weighted_Thread_Sum_Aggr/GROUPING_RESULTS
      ^-- Priti could help on this analysis task  
-----------------------------------------------------------------------------------------------------------------------------------

[Priority-3] Incorporate n-hop information flow with directed graph, similar to the neighborhood aggregation in GNN.
   
   -->  Refer to the neighborhood aggregation code of, say, 'pytorch GIN-conv layer' 

   ---> Check how the node embeddings get generalized for basic neighborhood-aggregation(message-passing)?

-----------------------------------------------------------------------------------------------------------------------------------
[Priority-4] Do alittle more analysis on "Investigating the reason using SHAP explainer" to see if can get clue?
    
    --> Instead of writing a script that also prints "feature value with feature shap",
        for mispredicted samples, could compare the waterfall plots of with and without graph-embedding 
        to see how their top features (w.r.t feature shap) and corresponding feature values changed

    --> Question to ponder: Why does relying solely on the distribution of events as a feature (i.e. flattened-graph features) 
                            result in decent performance? 
                            (Are the train/test samples substantially similar, leading to such result?)

    --> Understanding the data ( do this in parallel )

-----------------------------------------------------------------------------------------------------------------------------------
[Priority-5] Incorporate "Local N-gram (temporal information local to pair of nodes)" to graph-embedding (i.e. 'N>1'gram between thread node and f/r/n/p node)
   -- This definitely will involve alot of work + considerations 
   -- May not be very scalable ; Prof Stoller once said to use explainer to figure out which n-gram features are less important
   -- fit_transform for all edges in training data , in parallel? , then aggregate , and transform? 
      (this will need some good amount of effort, LESS importance ** )

-----------------------------------------------------------------------------------------------------------------------------------

[ Priti is also running the "Signal-Amplification suggested approach suggested by Prof Guanhua way back" ]

-----------------------------------------------------------------------------------------------------------------------------------

< Think and add more >

GRNN?